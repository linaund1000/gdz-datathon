{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "from keras.models import Sequential # type: ignore\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "from keras_tuner import Hyperband # type: ignore\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df= pd.read_csv('../input/gdz-elektrik-datathon/train.csv')\n",
    "# test_df= pd.read_csv('../input/gdz-elektrik-datathon/test.csv')\n",
    "# holidays_df = pd.read_csv('../input/gdz-elektrik-datathon/holidays.csv')\n",
    "# weather_df= pd.read_csv('../input/gdz-elektrik-datathon/weather.csv')\n",
    "train_df= pd.read_csv('./train.csv')\n",
    "test_df= pd.read_csv('./test.csv')\n",
    "holidays_df = pd.read_csv('./holidays.csv')\n",
    "weather_df= pd.read_csv('./weather.csv')\n",
    "\n",
    "train_df['tarih'] = pd.to_datetime(train_df['tarih'])\n",
    "train_df['ilce'] = train_df['ilce'].astype('category')\n",
    "train_df[\"bildirimsiz_sum\"] = train_df[\"bildirimsiz_sum\"].astype(int)\n",
    "train_df[\"bildirimli_sum\"] = train_df[\"bildirimli_sum\"].astype(int)\n",
    "\n",
    "test_df['tarih'] = pd.to_datetime(test_df['tarih'])\n",
    "test_df['ilce'] = test_df['ilce'].astype('category')\n",
    "test_df[\"bildirimli_sum\"] = test_df[\"bildirimli_sum\"].astype(int)\n",
    "\n",
    "holidays_df[\"tarih\"] = holidays_df['Yıl'].astype(str) + '-' + holidays_df['Ay'].astype(str) + '-' + holidays_df['Gün'].astype(str)\n",
    "holidays_df[\"tarih\"] = pd.to_datetime(holidays_df[\"tarih\"])\n",
    "holidays_df = holidays_df.drop(columns=['Yıl', 'Ay', 'Gün'])\n",
    "\n",
    "\n",
    "weather_df[\"tarih\"] = pd.to_datetime(weather_df[\"date\"])\n",
    "weather_df['ilce'] = weather_df['name'].astype('category')\n",
    "weather_df = weather_df.drop(columns=['date','name'])\n",
    "#Train\n",
    "merged_train_df = pd.merge(train_df, holidays_df, on='tarih', how='left').reset_index()\n",
    "merged_train_df['Bayram_Flag'] = merged_train_df['Tatil Adı'].fillna(0)\n",
    "merged_train_df['Bayram_Flag'] = merged_train_df['Bayram_Flag'].astype('category')\n",
    "merged_train_df = merged_train_df.drop(columns=['Tatil Adı'])\n",
    "\n",
    "merged_train_df['is_Bayram'] = merged_train_df['Bayram_Flag'].apply(lambda x: 0 if x == 0 else 1)\n",
    "merged_train_df['is_Bayram'] = merged_train_df['Bayram_Flag'].astype(bool)\n",
    "merged_train_df['ilce']=merged_train_df['ilce'].astype('category')\n",
    "\n",
    "#Test\n",
    "merged_test_df = pd.merge(test_df, holidays_df, on='tarih', how='left').reset_index()\n",
    "merged_test_df['Bayram_Flag'] = merged_test_df['Tatil Adı'].fillna(0)\n",
    "merged_test_df['Bayram_Flag'] = merged_test_df['Bayram_Flag'].astype('category')\n",
    "merged_test_df = merged_test_df.drop(columns=['Tatil Adı'])\n",
    "\n",
    "merged_test_df['is_Bayram'] = merged_test_df['Bayram_Flag'].apply(lambda x: 0 if x == 0 else 1)\n",
    "merged_test_df['is_Bayram'] = merged_test_df['Bayram_Flag'].astype(bool)\n",
    "merged_test_df['ilce']=merged_test_df['ilce'].astype('category')\n",
    "#weather op\n",
    "daily_df = weather_df.groupby(['ilce', pd.Grouper(freq='D', key='tarih')])\n",
    "\n",
    "daily_df = daily_df.agg({\n",
    "    't_2m:C': ['max', 'min'],  # temperature\n",
    "    'prob_precip_1h:p': ['sum', 'max' ,'mean',lambda x: x.mode()[0]],  # precipitation\n",
    "    'wind_speed_10m:ms': ['max', 'mean','std',lambda x: x.mode()[0]],  # wind speed\n",
    "    'wind_dir_10m:d': 'mean',  # wind direction\n",
    "    'global_rad:W': 'sum',  # sunshine duration\n",
    "    'effective_cloud_cover:p': ['mean','std'],  # cloud cover\n",
    "    'relative_humidity_2m:p': ['max', 'min',lambda x: x.mode()[0]]  # humidity\n",
    "})\n",
    "\n",
    "daily_df.columns = ['_'.join(col).strip() for col in daily_df.columns.values]\n",
    "daily_df = daily_df.reset_index()\n",
    "daily_df = daily_df.rename(columns={col: col.replace('<lambda_0>', 'mode') for col in daily_df.columns})\n",
    "daily_df['ilce'] = daily_df['ilce'].str.lower()\n",
    "weather_df=daily_df\n",
    "\n",
    "#merging all\n",
    "merged_test_df = pd.merge(weather_df, merged_test_df, on=['tarih', 'ilce'], how='inner')\n",
    "merged_train_df = pd.merge(weather_df, merged_train_df, on=['tarih', 'ilce'], how='inner')\n",
    "\n",
    "merged_test_df['ilce'] = merged_test_df['ilce'].astype('category')\n",
    "merged_train_df['ilce'] = merged_train_df['ilce'].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m X, y \u001b[38;5;241m=\u001b[39m predict_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbildirimsiz_sum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbildirimsiz_sum\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     13\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m X_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(X_test)\n\u001b[0;32m     17\u001b[0m y_train \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(y_train)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    293\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[0;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "features = ['t_2m:C_max', 't_2m:C_min', 'prob_precip_1h:p_sum','prob_precip_1h:p_max', 'prob_precip_1h:p_mean',\n",
    "       'prob_precip_1h:p_mode', 'wind_speed_10m:ms_max','wind_speed_10m:ms_mean', 'wind_speed_10m:ms_std','wind_speed_10m:ms_mode',\n",
    "       'wind_dir_10m:d_mean', 'global_rad:W_sum','effective_cloud_cover:p_mean', 'effective_cloud_cover:p_std','relative_humidity_2m:p_max',\n",
    "       'relative_humidity_2m:p_min','relative_humidity_2m:p_mode', 'bildirimsiz_sum',\n",
    "       'bildirimli_sum','is_Bayram']\n",
    "\n",
    "grouped = merged_train_df.groupby('ilce')\n",
    "\n",
    "for district, group in grouped:\n",
    "    predict_df = group[features]\n",
    "    X, y = predict_df.drop('bildirimsiz_sum', axis=1), predict_df[['bildirimsiz_sum']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    X_train = tf.constant(X_train)\n",
    "    X_test = tf.constant(X_test)\n",
    "    y_train = tf.constant(y_train)\n",
    "    y_test = tf.constant(y_test)\n",
    "    # Reshape data for CNN\n",
    "    X_train = X_train.values.reshape(-1, X_train.shape[1], 1)\n",
    "    X_test = X_test.values.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "    def build_model(hp):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(\n",
    "                filters=hp.Choice('conv_filters', values=[16, 32, 48, 64, 80, 96, 112, 128]),\n",
    "                kernel_size=hp.Choice('conv_kernel_size', values=[2, 3, 4, 5]),\n",
    "                strides=hp.Choice('conv_strides_x', values=[1, 2]),\n",
    "                padding=hp.Choice('conv_padding', values=['valid', 'same']),\n",
    "                activation=hp.Choice('activation', values=['relu', 'tanh', 'swish']),\n",
    "                input_shape=(X_train.shape[1], 1)\n",
    "        ))\n",
    "        model.add(MaxPooling1D(\n",
    "                pool_size=hp.Choice('pool_size_choice', values=[2, 3]),\n",
    "                strides=hp.Choice('strides_choice', values=[2, 3])\n",
    "        ))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(\n",
    "                units=hp.Choice('dense_units', values=[32, 64, 96, 128, 160, 192, 224, 256]),\n",
    "                activation=hp.Choice('activation', values=['relu', 'tanh', 'swish'])\n",
    "        ))\n",
    "        model.add(Dropout(rate=hp.Choice('dropout_rate', values=[0.1, 0.2, 0.3, 0.4, 0.5])))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.compile(\n",
    "                loss='mean_squared_error',\n",
    "                optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'adagrad']),\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='/tmp/keras-tuner-hyperband',\n",
    "    objective='val_mean_absolute_error'\n",
    "    )\n",
    "    \n",
    "    tuner.search(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Evaluate the best model\n",
    "    preds = best_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds, squared=False)\n",
    "    print(f\"MAE for {district}: {mae}\")\n",
    "\n",
    "#modeldeki hatayı çözemedim chatgpt saolsun o da çözemedi :D \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
